[
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html",
    "href": "posts/2022-10-25-credit-card-approvals.html",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "",
    "text": "Bank komersial mendapatkan banyak form aplikasi untuk membuat kartu kredit. Permintaan kartu kredit ditolak dengan berbagai alasan, contohnya, jumlah hutang yang tinggi, pendapatan rendah, atau masalah lain pada laporan kredit. Memeriksa semua form aplikasi secara manual menghabiskan banyak waktu, dan manusia cenderung membuat kesalahan. Pada artikel ini saya akan mencoba membuat model machine learning untuk memprediksi kelayakan aplikasi kartu kedit secara otomatis\n\nBerikut merupakan dataset “Credit Card Approval” dari UCI Machine Learning Repository.\n\n# Import pandas\nimport pandas as pd\n# Load dataset\ncc_apps = pd.read_csv(\"datasets/cc_approvals.data\", header=None)\ncc_apps.head()"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#aplikasi-kartu-kredit",
    "href": "posts/2022-10-25-credit-card-approvals.html#aplikasi-kartu-kredit",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "2. Aplikasi kartu kredit",
    "text": "2. Aplikasi kartu kredit\nSemua fitur pada dataset ini tidak diperlihatkan untuk menjaga privasi, namun blog ini memberikan penjelasan yang cukup baik mengenai fitur-fitur yang mungkin dapat digunakan. Fitur-fitur yang biasa dadapati pada aplikasi kartu kredit adalah Gender, Age, Debt, Married, BankCustomer, EducationLevel, Ethnicity, YearsEmployed, PriorDefault, Employed, CreditScore, DriversLicense, Citizen, ZipCode, Income dan ApprovalStatus. Saat ini dataset ini hanya kumpulan fitur numerik dan non-numerikal, Masalah ini dapat diperbaiki dengan preprocessing, namun sebelum melakukan preprocessing, sebaiknya kita harus memeriksa semua masalah yang mungkin ada pada dataset ini yang perlu diperbaiki.\n\n# Print summary statistics\ncc_apps_description = cc_apps.describe()\nprint(cc_apps_description)\n\nprint('\\n')\n\n# Print DataFrame information\ncc_apps_info = cc_apps.info()\nprint(cc_apps_info)\n\nprint('\\n')\nprint(cc_apps.tail(17))\n\n# Inspect missing values in the dataset\n# ... YOUR CODE FOR TASK 2 ...\n\n               2           7          10             14\ncount  690.000000  690.000000  690.00000     690.000000\nmean     4.758725    2.223406    2.40000    1017.385507\nstd      4.978163    3.346513    4.86294    5210.102598\nmin      0.000000    0.000000    0.00000       0.000000\n25%      1.000000    0.165000    0.00000       0.000000\n50%      2.750000    1.000000    0.00000       5.000000\n75%      7.207500    2.625000    3.00000     395.500000\nmax     28.000000   28.500000   67.00000  100000.000000\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 690 entries, 0 to 689\nData columns (total 16 columns):\n0     690 non-null object\n1     690 non-null object\n2     690 non-null float64\n3     690 non-null object\n4     690 non-null object\n5     690 non-null object\n6     690 non-null object\n7     690 non-null float64\n8     690 non-null object\n9     690 non-null object\n10    690 non-null int64\n11    690 non-null object\n12    690 non-null object\n13    690 non-null object\n14    690 non-null int64\n15    690 non-null object\ndtypes: float64(2), int64(2), object(12)\nmemory usage: 86.3+ KB\nNone\n\n\n    0      1       2  3  4   5   6      7  8  9   10 11 12     13   14 15\n673  ?  29.50   2.000  y  p   e   h  2.000  f  f   0  f  g  00256   17  -\n674  a  37.33   2.500  u  g   i   h  0.210  f  f   0  f  g  00260  246  -\n675  a  41.58   1.040  u  g  aa   v  0.665  f  f   0  f  g  00240  237  -\n676  a  30.58  10.665  u  g   q   h  0.085  f  t  12  t  g  00129    3  -\n677  b  19.42   7.250  u  g   m   v  0.040  f  t   1  f  g  00100    1  -\n678  a  17.92  10.210  u  g  ff  ff  0.000  f  f   0  f  g  00000   50  -\n679  a  20.08   1.250  u  g   c   v  0.000  f  f   0  f  g  00000    0  -\n680  b  19.50   0.290  u  g   k   v  0.290  f  f   0  f  g  00280  364  -\n681  b  27.83   1.000  y  p   d   h  3.000  f  f   0  f  g  00176  537  -\n682  b  17.08   3.290  u  g   i   v  0.335  f  f   0  t  g  00140    2  -\n683  b  36.42   0.750  y  p   d   v  0.585  f  f   0  f  g  00240    3  -\n684  b  40.58   3.290  u  g   m   v  3.500  f  f   0  t  s  00400    0  -\n685  b  21.08  10.085  y  p   e   h  1.250  f  f   0  f  g  00260    0  -\n686  a  22.67   0.750  u  g   c   v  2.000  f  t   2  t  g  00200  394  -\n687  a  25.25  13.500  y  p  ff  ff  2.000  f  t   1  t  g  00200    1  -\n688  b  17.92   0.205  u  g  aa   v  0.040  f  f   0  f  g  00280  750  -\n689  b  35.00   3.375  u  g   c   h  8.290  f  f   0  t  g  00000    0  -"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#membagi-data-menjadi-train-dan-test-set",
    "href": "posts/2022-10-25-credit-card-approvals.html#membagi-data-menjadi-train-dan-test-set",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "3. Membagi data menjadi train dan test set",
    "text": "3. Membagi data menjadi train dan test set\nSekarang, saya akan membagi data menjadi 2 set yaitu train set dan test set. Untuk mempersiapkan data untuk dua tahapan lain dari proses machine learning yaitu training dan testing. Sebaiknya, tidak ada informasi pada test set yang digunakan untuk membantu proses preprocessing pada train set. maka dari itu pertama saya akan membagi data terlebih dahulu kemudian membaginya. Selain itu fitur-fitur seperti DriversLicense dan ZipCode tidak terlalu penting untuk digunakan pada proses training dibandingkan dengan fitur-fitur yang lain untuk memprediksi kelayakan aplikasi kartu kredit\n\n# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Drop fitur 11 dan 13\ncc_apps = cc_apps.drop([11,13], axis=1)\n\n# Split data menjadi train dan test set\ncc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#mengatasi-missing-values-part-i",
    "href": "posts/2022-10-25-credit-card-approvals.html#mengatasi-missing-values-part-i",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "4. Mengatasi missing values (part i)",
    "text": "4. Mengatasi missing values (part i)\nSetelah data dibagi menjadi train dan test set. kita bisa mulai mengatasi masalah yang kita temui ketika memeriksa dataframe:\n\n\nDataset yang digunakan saat ini memiliki data numerik dan non-numerik(float64, int64 dan object)\n\n\nDataset ini juga memiliki berbagai rentang nilai. Ada fitur memiliki rentang nilai 0-28, ada juga yang memiliki rentang 2-67, bahkan 1017-100000.\n\n\nYang terakhir pada dataset beberapa baris pada setiap kolom tidak memiliki nilai (missing values). Missing values pada dataset ditandai dengan simbol ?\n\n\n\n# Import numpy\nimport numpy as np \n\n# Mengganti simbol ? dengan NaN pada train dan test set \ncc_apps_train = cc_apps_train.replace('?',np.NaN)\ncc_apps_test = cc_apps_test.replace('?',np.NaN)"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#mengatasi-missing-values-part-ii",
    "href": "posts/2022-10-25-credit-card-approvals.html#mengatasi-missing-values-part-ii",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "5. Mengatasi missing values (part ii)",
    "text": "5. Mengatasi missing values (part ii)\nSemua ? sudah diganti dengan NaN. Ini bertujuan untuk membantu kita untuk pada tahapan selanjutnya untuk mengatasi missing value, yaitu dengan strategi mean imputation\n\n# Imputasi missing values dengan mean imputation\ncc_apps_train.fillna(cc_apps_train.mean(), inplace=True)\ncc_apps_test.fillna(cc_apps_test.mean(), inplace=True)\n\n# Tampilkan jumlah nilai NaN pada dataset untuk memverifikasi\nprint(cc_apps_train.isnull().sum())\nprint(cc_apps_test.isnull().sum())"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#mengatasi-missing-values-part-iii",
    "href": "posts/2022-10-25-credit-card-approvals.html#mengatasi-missing-values-part-iii",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "6. Mengatasi missing values (part iii)",
    "text": "6. Mengatasi missing values (part iii)\nKita telah berhasi mengatasi missing values yang ada pada kolom yang memiliki nilai numerikas. namun masih banyak missing values yang perlu di imputasi pada kolom 0, 1, 3, 4, 5, 6, dan 13. Semua kolom ini memiliki nilai non-numerik, maka dari itu strategi mean imputation tidak akan berhasil jika digunakan pada kolom tersebut. Imputasi pada kolom-kolom ini akan dilakukan dengan menggunakan nilai yang sering muncul pada kolom tersebut. Metode ini secara umum baik digunakan ketika melakukan imputasi untuk data kategorikal.\n\n# Iterasi untuk setiap kolom pada cc_apps_train \nfor col in cc_apps_train.columns:\n    # Jika tipe data kolom tersebut object\n    if cc_apps_train[col].dtypes == 'object':\n        # Imputasi dengan nilai yang paling sering muncul\n        cc_apps_train = cc_apps_train.fillna(cc_apps_train[col].value_counts().index[0])\n        cc_apps_test = cc_apps_test.fillna(cc_apps_train[col].value_counts().index[0])\n\n# Hitung jumlah nilai NaN yang ada pada dataset \nprint(cc_apps_train.isnull().sum())\nprint(cc_apps_test.isnull().sum())\n\n0     0\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9     0\n10    0\n12    0\n14    0\n15    0\ndtype: int64\n0     0\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9     0\n10    0\n12    0\n14    0\n15    0\ndtype: int64"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#data-preprocessing-part-i",
    "href": "posts/2022-10-25-credit-card-approvals.html#data-preprocessing-part-i",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "7. Data preprocessing (part i)",
    "text": "7. Data preprocessing (part i)\nSemua missing values telah berhasil diatasi. Masih ada beberapa langkah data preprocessing yang diperlukan sebelum mulai membangun model machine learning. Saya akan membagi tahapan preprocessing ini menjadi dua tahapan:\n\n\nUbah data non-numerik menjadi numerik.\n\n\nManipulasi data sehingga semua nilai fitur memiliki rentang nilai yang sama\n\n\nPertama, saya akan mengubah semua nilai non-numerik menjadi numerik. Hal ini penting dilakukan untuk mendapatkan proses komputasi yang lebih cepat, selain itu banyak model machine learning (terutama yang ada pada library scikit-learn) mengharuskan data dengan format numerik. Hal ini akan dilakukan dengan menggunakan method get_dummies() dari library pandas.\n\npd.get_dummies(cc_apps_train)\n\n\n\n\n\n  \n    \n      \n      2\n      7\n      10\n      14\n      0_a\n      0_b\n      1_13.75\n      1_15.83\n      1_15.92\n      1_16.00\n      ...\n      6_z\n      8_f\n      8_t\n      9_f\n      9_t\n      12_g\n      12_p\n      12_s\n      15_+\n      15_-\n    \n  \n  \n    \n      382\n      2.500\n      4.500\n      0\n      456\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      137\n      2.750\n      4.250\n      6\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      346\n      1.500\n      0.250\n      0\n      122\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      326\n      1.085\n      0.040\n      0\n      179\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      33\n      5.125\n      5.000\n      0\n      4000\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n    \n    \n      426\n      2.040\n      0.250\n      0\n      50\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      477\n      2.500\n      10.000\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      9\n      4.915\n      3.165\n      0\n      1442\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n    \n    \n      22\n      8.000\n      7.875\n      6\n      1260\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      300\n      2.000\n      6.500\n      1\n      10\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      221\n      11.000\n      20.000\n      7\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      398\n      12.500\n      1.250\n      0\n      17\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      675\n      1.040\n      0.665\n      0\n      237\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      550\n      22.290\n      12.750\n      1\n      109\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      84\n      0.625\n      0.455\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      331\n      2.500\n      2.500\n      0\n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      332\n      2.500\n      1.000\n      0\n      16\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      578\n      1.625\n      1.500\n      10\n      4700\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      259\n      1.750\n      0.165\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      203\n      10.250\n      0.710\n      2\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      196\n      3.165\n      3.165\n      3\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      651\n      7.625\n      0.125\n      1\n      160\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      513\n      9.960\n      0.000\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n    \n    \n      393\n      0.540\n      1.000\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      93\n      1.375\n      9.460\n      0\n      100\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      623\n      0.000\n      0.665\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      291\n      10.000\n      1.000\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      299\n      12.125\n      3.335\n      2\n      173\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      369\n      0.750\n      0.750\n      0\n      2\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      184\n      5.665\n      2.585\n      7\n      3257\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      252\n      11.000\n      1.000\n      11\n      3000\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      21\n      1.000\n      0.835\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n    \n    \n      313\n      10.000\n      0.415\n      0\n      42\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      459\n      0.290\n      1.500\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      160\n      2.000\n      1.000\n      4\n      7544\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      276\n      5.415\n      0.290\n      0\n      484\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      191\n      0.205\n      5.125\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n    \n    \n      385\n      5.500\n      1.500\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      413\n      1.500\n      0.000\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      491\n      0.875\n      4.625\n      2\n      2000\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      343\n      2.750\n      0.000\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      308\n      1.290\n      0.250\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      661\n      3.500\n      0.165\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      130\n      5.500\n      13.000\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      663\n      4.000\n      1.500\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      99\n      1.000\n      1.000\n      2\n      500\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      372\n      4.585\n      1.000\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      87\n      2.210\n      4.000\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      458\n      5.500\n      5.000\n      0\n      687\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      330\n      0.000\n      0.000\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      214\n      2.710\n      5.250\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      466\n      3.085\n      2.500\n      2\n      41\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      121\n      12.500\n      1.210\n      67\n      258\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      614\n      4.415\n      0.125\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      20\n      11.250\n      2.500\n      17\n      1208\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      71\n      4.000\n      12.500\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      106\n      1.165\n      0.500\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      270\n      0.000\n      0.000\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n    \n    \n      435\n      0.000\n      0.000\n      4\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      102\n      5.000\n      0.375\n      2\n      38\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n  \n\n462 rows × 334 columns\n\n\n\n\n# Ubah fitur pada train dan test set secara terpisah\ncc_apps_train = pd.get_dummies(cc_apps_train)\ncc_apps_test = pd.get_dummies(cc_apps_test)\n\n# Reindex kolom-kolom yang ada pada test set agar sejajar dengan train set\ncc_apps_test = cc_apps_test.reindex(columns=cc_apps_train.columns, fill_value=0)\n\n\ncc_apps_train.iloc[:, :-1].values, cc_apps_train.iloc[:, [-1]].values\n\n(array([[2.5  , 4.5  , 0.   , ..., 0.   , 0.   , 0.   ],\n        [2.75 , 4.25 , 6.   , ..., 0.   , 0.   , 1.   ],\n        [1.5  , 0.25 , 0.   , ..., 0.   , 0.   , 0.   ],\n        ...,\n        [0.   , 0.   , 0.   , ..., 1.   , 0.   , 1.   ],\n        [0.   , 0.   , 4.   , ..., 0.   , 0.   , 0.   ],\n        [5.   , 0.375, 2.   , ..., 0.   , 0.   , 0.   ]]),\n array([[1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [0],\n        [1],\n        [1],\n        [0],\n        [1],\n        [1]], dtype=uint8))"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#data-preprocessing-part-ii",
    "href": "posts/2022-10-25-credit-card-approvals.html#data-preprocessing-part-ii",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "8. Data preprocessing (part ii)",
    "text": "8. Data preprocessing (part ii)\nProses preprocessing terakhir adalah mengubah rentang nilai data (Rescaling). untuk menjelaskan proses rescaling saya gunakan contoh fitur CreditScore. Credit score seseorang adalah sebuah nilai kelayakan seseorang untuk memiliki kredit berdasarkan credit history mereka. Semakin tinggi credit score seseorang maka secara finansial mereka akan semakin dipercayai untuk memiliki kredit. Jadi, saya akan melakukan rescaling pada fitur CreditScore dan fitur-fitur lain sehingga memiliki rentang nilai dari 0-1 dimana 0 adalah nilai terendah dan 1 adalah nilai tertinggi\n\n# Import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Pisahkan fitur dan label ke variabel yang berbeda\nX_train, y_train = cc_apps_train.iloc[:, :-1].values, cc_apps_train.iloc[:, [-1]].values\nX_test, y_test = cc_apps_test.iloc[:, :-1].values, cc_apps_test.iloc[:, [-1]].values\n\n# Gunakan MinMaxScaler untuk rescaling X_train dan X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#fitting-model-logistic-regression-dengan-train-set",
    "href": "posts/2022-10-25-credit-card-approvals.html#fitting-model-logistic-regression-dengan-train-set",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "9. Fitting model logistic regression dengan train set",
    "text": "9. Fitting model logistic regression dengan train set\nPada dasarnya, memprediksi jika aplikasi kartu kredit akan diterima atau tidak termasuk dalam jenis kasus klasifikasi. Menurut UCI, dataset yang saya gunakan saat ini memiliki lebih banyak aplikasi yang berstatus “Denied” daripada yang berstatus “Approved”. Dari 690 baris ada 383(55.5%) aplikasi dengan status “Denied” dan 307(44.5%) dengan status “Approved”. Model machine learning yang baik seharusnya dapat secara akurat memprediksi status aplikasi sesuai dengan statistik tersebut.\nModel machine learning mana yang sebaiknya kita pilih?. Karena semua kolom yang ada pada dataset ini memiliki korelasi yaitu sebagai nilai ukur layak atau tidaknya sebuah apliksi kartu kredit, saya akan memilih menggunakan Logistic Regression karena model tersebut biasanya memberikan hasil yang baik pada kasus serupa.\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Inisiasi LogisticRegression classifier dengan menggunakan nilai parameter default \nlogreg = LogisticRegression()\n\n# fit LogisticRegression ke train set\nlogreg.fit(rescaledX_train,y_train)\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#membuat-prediksi-dan-mengevaluasi-model-machine-learning",
    "href": "posts/2022-10-25-credit-card-approvals.html#membuat-prediksi-dan-mengevaluasi-model-machine-learning",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "10. Membuat prediksi dan mengevaluasi model machine learning",
    "text": "10. Membuat prediksi dan mengevaluasi model machine learning\nSelanjutnya adalah mengukur seberapa baik performa model machine learning yang telah dibuat.\nSaya akan mengevaluasi model ini dengan test set untuk mengukur classification accuracy. Tapi sebelum itu saya juga akan melihat confusion matrix dari model yang telah dibuat. Pada kasus memprediksi kelayakan aplikasi kartu kredit, penting untuk melihat apakah model yang telah dibuat telah berhasil memprediksi status “Approved” dan “Denied” yang setara dengan frekuensi label pada dataset awal. Jika hasilnya tidak sesuai dengan aspek tersebut, maka mungkin saja model yang dibuat nantinya akan memberikan status “Approved” kepada aplikasi yang seharusnya diberikan status denied. Confusion Matrix akan membantu kita untuk memperlihatkan kesesuaian aspek tersebutk.\n\n# Import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Gunakan logistic regression untuk memprediksi semua data pada test set dan simpan pada variable y_pred\ny_pred = logreg.predict(rescaledX_test)\n\n# Print nilai akurasi dari model logistic regression\nprint(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test,y_test))\n\n# Print confusion matrix dari model logistic regression\nconfusion_matrix(y_test,y_pred)\n\nAccuracy of logistic regression classifier:  1.0\n\n\narray([[103,   0],\n       [  0, 125]])"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#grid-searching-dan-meningkatkan-performa-model-machine-learning",
    "href": "posts/2022-10-25-credit-card-approvals.html#grid-searching-dan-meningkatkan-performa-model-machine-learning",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "11. Grid searching dan meningkatkan performa model machine learning",
    "text": "11. Grid searching dan meningkatkan performa model machine learning\nHasil prediksi model cukup baik, bahkan dapat mencapai nilai akurasi 100%.\nPada confusion matrix, element pertama dari baris pertama confusion matrix menunjukan “true negatives” artinya jumlah aplikasi yang ditolak oleh model dan memang seharusnya ditolak. element terakhir dari baris kedua confusion matrix menunjukan “true positives” artinya jumlah aplikasi yang ditolak oleh model dan memang seharusnya ditolak.\nModel machine learning yang dihasilkan sangat baik, namun jika model machine learning mendapatkan performa yang tidak sesuai harapan hal yang bisa dilakukan adalah dengan melakukan grid search pada parameter model yang digunakan untuk mencari parameter yang sesuai agar model mendapatkan kemampuan prediksi yang baik.\nImplementasi logistic regression terdiri dari beberapa hyperparameter namun saya akan melakukan grid search pada dua hyperparameter berikut\n\n\ntol\n\n\nmax_iter\n\n\n\n# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Tentukan nilai-nilai yang akan digunakan untuk tol, dan max_iter\ntol = [0.01, 0.001 ,0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\n# Buat dictionary dimana tol dan max_iter adalah key dan value-nya adalah list dari nilai-nilai yang telah ditentukan sebelumnya pada masing-masing key\nparam_grid = dict(tol=tol, max_iter=max_iter)"
  },
  {
    "objectID": "posts/2022-10-25-credit-card-approvals.html#menentukan-model-dengan-performa-yang-paling-baik",
    "href": "posts/2022-10-25-credit-card-approvals.html#menentukan-model-dengan-performa-yang-paling-baik",
    "title": "Klasifikasi Persetujuan Permohonan Aplikasi Kartu Kredit",
    "section": "12. Menentukan model dengan performa yang paling baik",
    "text": "12. Menentukan model dengan performa yang paling baik\nGrid dari nilai hyperparameter telah ditentukan dan telah dibuat dalam bentuk dictionary yang mana akan digunakan GridSearchCV() sebagai salah satu dari parameterny. Sekarang, mulai grid search untuk melihat nilai mana yang memberikan performa terbaik.\n\n\nPertama saya akan menginisiasi GridSearchCV() dengan model logistic regression sebelumnya dengan data yang dimiliki.\n\n\nKemudia yang terakhir menyimpan skor terbaik dan nilai parameter yang digunakan\n\n\n\n# Inisiasi GridSearchCV dengan semua parameter yang diperlukan\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model ke data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Tampilkan hasil\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\n# Gunakan model terbaik dan evaluasi pada test set\nbest_model = grid_model_result.best_estimator_\nprint(\"Accuracy of logistic regression classifier: \", best_model.score(rescaledX_test,y_test))\n\nBest: 1.000000 using {'max_iter': 100, 'tol': 0.01}\nAccuracy of logistic regression classifier:  1.0"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html",
    "href": "posts/2020-01-14-test-markdown-post.html",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Footnotes\n\n\nThis is the footnote.↩︎"
  },
  {
    "objectID": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html",
    "href": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html",
    "title": "EDA Pada Kasus Pemulihan Hutang Bank",
    "section": "",
    "text": "Setelah suatu utang dinyatakan secara hukum “tidak dapat ditagih” oleh bank, rekening tersebut dianggap “hilang”. Tapi itu tidak berarti bank melupakan utang tersebut. Mereka masih ingin mengumpulkan sebagian dari uang yang mereka miliki. Bank akan menilai akun untuk mengevaluasi “Expected Recovery Amount”, yaitu jumlah uang yang diharapkan yang mungkin dapat diterima bank dari pelanggan di masa depan. Jumlah ini merupakan fungsi dari probabilitas pelanggan membayar, total hutang, dan faktor lain yang mempengaruhi kemampuan dan kemauan pelanggan membayar.\n\n\nBank telah menerapkan strategi pemulihan yang berbeda pada ambang batas yang berbeda ($1000, $2000, dll.) di mana semakin besar jumlah pemulihan yang diharapkan, semakin banyak upaya yang dilakukan bank untuk menghubungi pelanggan. Untuk jumlah pemulihan yang rendah (Level 0), bank hanya menambahkan informasi kontak pelanggan ke dialer otomatis dan sistem email mereka. Untuk strategi pemulihan yang lebih tinggi, bank mengeluarkan lebih banyak biaya karena mereka memanfaatkan sumber daya manusia dalam lebih banyak upaya untuk mendapatkan pembayaran. Setiap level tambahan dari strategi pemulihan memerlukan tambahan $50 per pelanggan sehingga pelanggan di Strategi Pemulihan Level 1 membebani perusahaan $50 lebih banyak daripada pelanggan di Level 0. Pelanggan di Level 2 berharga $50 lebih mahal daripada pelanggan di Level 1, dst.\n\n\nPertanyaan besar: apakah jumlah ekstra yang diperoleh pada level yang lebih tinggi dari biaya $50 yang dikeluarkan? Dengan kata lain, apakah ada lompatan (juga disebut “discontinuity”) lebih dari $50 dalam jumlah yang dipulihkan pada tingkat level strategi yang lebih tinggi? Saya akan mencoba menjawab pertanyaan tersebut disini.\n\n\nPertama, kita akan memuat dataset perbankan dan melihat beberapa baris data pertama. Untuk memahami kumpulan data itu sendiri dan mulai memikirkan cara menganalisis data.\n\n\n# Import modules\nimport pandas as pd\nimport numpy as np\n# load dataset ke pandas dataframe\ndf = pd.read_csv(\"datasets/bank_data.csv\")\n# Tampilkan 5 baris pertama dataframe\ndf.head()\n\n\n\n\n\n  \n    \n      \n      id\n      expected_recovery_amount\n      actual_recovery_amount\n      recovery_strategy\n      age\n      sex\n    \n  \n  \n    \n      0\n      2030\n      194\n      263.540\n      Level 0 Recovery\n      19\n      Male\n    \n    \n      1\n      1150\n      486\n      416.090\n      Level 0 Recovery\n      25\n      Female\n    \n    \n      2\n      380\n      527\n      429.350\n      Level 0 Recovery\n      27\n      Male\n    \n    \n      3\n      1838\n      536\n      296.990\n      Level 0 Recovery\n      25\n      Male\n    \n    \n      4\n      1995\n      541\n      346.385\n      Level 0 Recovery\n      34\n      Male"
  },
  {
    "objectID": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#graphical-exploratory-data-analysis",
    "href": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#graphical-exploratory-data-analysis",
    "title": "EDA Pada Kasus Pemulihan Hutang Bank",
    "section": "2. Graphical exploratory data analysis",
    "text": "2. Graphical exploratory data analysis\n\nBank telah menerapkan strategi pemulihan yang berbeda pada ambang batas yang berbeda ($1000, $2000, $3000 dan $5000) di mana semakin besar Jumlah Pemulihan yang Diharapkan, semakin banyak upaya yang dilakukan bank untuk menghubungi pelanggan. Menekankan pada transisi pertama (antara Level 0 dan Level 1) berarti kita berfokus pada populasi dengan “Expected Recovery Amounts” yang Diharapkan antara $0 dan $2000 di mana transisi artar Level terjadi pada $1000. Pelanggan di Level 1 (“Expected Recovery Amounts” antara $1001 dan $2000) menerima lebih banyak perhatian dari bank dan, menurut definisi, mereka memiliki Jumlah Pemulihan yang Diharapkan lebih tinggi daripada pelanggan di Level 0 (antara $1 dan $1000) .\n\n\nBerikut ini ringkasan singkat tentang Level dan ambang batas lagi:\n\n\n\nLevel 0: Jumlah pemulihan yang diharapkan >$0 dan <=$1000\n\n\nLevel 1: Jumlah pemulihan yang diharapkan >$1000 dan <=$2000\n\n\nThreshold $1000 memisahkan Level 0 dari Level 1\n\n\n\nPertanyaan kuncinya adalah apakah ada faktor lain selain Jumlah Pemulihan yang Diharapkan yang juga bervariasi secara sistematis di ambang $1000. Misalnya, apakah usia pelanggan menunjukkan lompatan (diskontinuitas) pada ambang $1000 atau apakah usia itu bervariasi? Kita dapat memeriksa ini dengan terlebih dahulu membuat scatter plot usia sebagai fungsi dari “Expected Recovery Amounts” untuk rentang kecil “Expected Recovery Amounts”, $0 hingga $2000. Rentang ini mencakup Level 0 dan 1.\n\n\n# Scatter plot Age vs. Expected Recovery Amount\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nplt.scatter(x=df['expected_recovery_amount'], y=df['age'], c=\"g\", s=2)\nplt.xlim(0, 2000)\nplt.ylim(0, 60)\nplt.xlabel( \"Expected Recovery Amount\")\nplt.ylabel(\"Age\")\nplt.legend(loc=2)\n\n<matplotlib.legend.Legend at 0x7f873c306f28>\n\n\n/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  warnings.warn(\"This figure includes Axes that are not compatible \""
  },
  {
    "objectID": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#statistical-test-age-vs.-expected-recovery-amount",
    "href": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#statistical-test-age-vs.-expected-recovery-amount",
    "title": "EDA Pada Kasus Pemulihan Hutang Bank",
    "section": "3. Statistical test: age vs. expected recovery amount",
    "text": "3. Statistical test: age vs. expected recovery amount\n\nKita ingin memastikan bahwa variabel seperti usia dan jenis kelamin serupa di atas ataupun di bawah threshold $1000 “Expected Recovery Amount”. Ini penting karena kita ingin dapat menyimpulkan bahwa perbedaan “Expected Recovery Amount” dan jumlah pemulihan sebenarnya (Actual Recovery Amount) disebabkan oleh Strategi Pemulihan yang lebih tinggi dan bukan karena perbedaan lain seperti usia atau jenis kelamin.\n\n\nPlot sebar usia dan “Expected Recovery Amount” tidak menunjukkan lompatan yang jelas sekitar $1000. Selanjutnya akan dilakukan analisis statistik yang memeriksa usia rata-rata pelanggan yang berada hampir tepat di atas dan di bawah ambang batas. Kita bisa mulai dengan memeriksa kisaran dari $900 hingga $1100.\n\n\nUntuk menentukan apakah ada perbedaan usia tepat di atas dan di bawah ambang batas, akan digunakan uji Kruskal-Wallis, uji statistik yang tidak membuat asumsi distribusi.\n\n\n# Import stats module dari library scipy\nfrom scipy import stats\n\n# Compute average age just below and above the threshold\n# Hitung rata-rata usia pelanggan yang berada disekitar bagian atas dan bagian bawah threshold\nera_900_1100 = df.loc[(df['expected_recovery_amount']<1100) & \n                      (df['expected_recovery_amount']>=900)]\nby_recovery_strategy = era_900_1100.groupby(['recovery_strategy'])\nby_recovery_strategy['age'].describe().unstack()\n\n# Lakukan Kruskal-Wallis test \nLevel_0_age = era_900_1100.loc[df['recovery_strategy']==\"Level 0 Recovery\"]['age']\nLevel_1_age = era_900_1100.loc[df['recovery_strategy']==\"Level 1 Recovery\"]['age']\nstats.kruskal(Level_0_age, Level_1_age) \n\nKruskalResult(statistic=3.4572342749517513, pvalue=0.06297556896097407)"
  },
  {
    "objectID": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#statistical-test-sex-vs.-expected-recovery-amount",
    "href": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#statistical-test-sex-vs.-expected-recovery-amount",
    "title": "EDA Pada Kasus Pemulihan Hutang Bank",
    "section": "4. Statistical test: sex vs. expected recovery amount",
    "text": "4. Statistical test: sex vs. expected recovery amount\n\nTerlihat bahwa tidak ada lompatan besar dalam rata-rata usia pelanggan yang berada disekitar threshold $1000 bagian atas dan bawah dengan melakukan uji statistik serta memeriksa secara grafis dengan scatter plot.\n\n\nKita juga sebaiknya menguji bahwa persentase pelanggan laki-laki tidak melampaui ambang $1000. Kita bisa mulai dengan menjelajahi kisaran $900 hingga $1100 dan kemudian menyesuaikan kisaran ini.\n\n\nKita dapat memeriksa pertanyaan ini secara statistik dengan mengembangkan cross-tabs serta melakukan chi-square tests dari persentase pelanggan yang laki-laki dan perempuan.\n\n\n# Jumlah pelanggan pada setiap kategori\ncrosstab = pd.crosstab(df.loc[(df['expected_recovery_amount']<1100) & \n                              (df['expected_recovery_amount']>=900)]['recovery_strategy'], \n                       df['sex'])\nprint(crosstab)\n# Chi-square test\nchi2_stat, p_val, dof, ex = stats.chi2_contingency(crosstab)\nprint(p_val)\n\nsex                Female  Male\nrecovery_strategy              \nLevel 0 Recovery       32    57\nLevel 1 Recovery       39    55\n0.5377947810444592"
  },
  {
    "objectID": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#exploratory-graphical-analysis-recovery-amount",
    "href": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#exploratory-graphical-analysis-recovery-amount",
    "title": "EDA Pada Kasus Pemulihan Hutang Bank",
    "section": "5. Exploratory graphical analysis: recovery amount",
    "text": "5. Exploratory graphical analysis: recovery amount\n\nSekarang kita sudah cukup yakin bahwa pelanggan yang berada tepat di atas dan di bawah threshold $1000, rata-rata, memiliki usia rata-rata dan persentase laki-laki.\n\n\nSekarang saatnya untuk fokus pada tujuan utama, jumlah pemulihan yang sebenarnya (Actual Recovery Amount).\n\n\nLangkah pertama dalam memeriksa hubungan antara Actual Recovery Amount dan Expected Recovery Amount adalah membuat scatter plot di mana kita ingin memfokuskan perhatian kita pada kisaran threshold bawah dan atas. Secara khusus, akan dibuat scatter plot Jumlah Expected Recovery Amount (X) dan Actual Recovery Amount (Y) untuk Actual Recovery Amount diantara $900 hingga $1100. Rentang ini mencakup Level 0 dan 1. Pertanyaan kuncinya adalah apakah kita melihat diskontinuitas (lompatan) di sekitar ambang $1000.\n\n\n# Scatter plot of Actual Recovery Amount vs. Expected Recovery Amount \nplt.scatter(x=df['expected_recovery_amount'], y=df['actual_recovery_amount'], c=\"g\", s=2)\nplt.xlim(900, 1100)\nplt.ylim(0, 2000)\nplt.xlabel(\"Expected Recovery Amount\")\nplt.ylabel(\"Actual Recovery Amount\")\nplt.legend(loc=2)\nplt.show\n\n<function matplotlib.pyplot.show(*args, **kw)>\n\n\n/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  warnings.warn(\"This figure includes Axes that are not compatible \""
  },
  {
    "objectID": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#statistical-analysis-recovery-amount",
    "href": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#statistical-analysis-recovery-amount",
    "title": "EDA Pada Kasus Pemulihan Hutang Bank",
    "section": "6. Statistical analysis: recovery amount",
    "text": "6. Statistical analysis: recovery amount\n\nSeperti yang telah dilakukan dengan variable usia, kita juga dapat melakukan uji statistik untuk melihat apakah jumlah pemulihan yang sebenarnya memiliki diskontinuitas di atas ambang $1000. Ini akan dilakukan untuk dua rentang berbeda dari jumlah pemulihan yang diharapkan $900 hingga $1100 dan untuk kisaran sempit $950 hingga $1050 untuk melihat apakah hasilnya konsisten.\n\n\nSekali lagi, kita akan menggunakan tes Kruskal-Wallis.\n\n\nPertama-tama, akan dihitung jumlah pemulihan aktual rata-rata untuk pelanggan tersebut tepat di bawah dan tepat di atas threshold menggunakan kisaran dari $900 hingga $1100. Kemudian akan dilakukan uji Kruskal-Wallis untuk melihat apakah jumlah pemulihan sebenarnya berbeda tepat di atas dan di bawah threshold. Setelah itu, langkah-langkah ini akan diulangi untuk rentang yang lebih kecil dari $950 hingga $1050.\n\n\n# Hitung rata-rata actual recovery amount yang ada diatas dan dibawah threshold\nby_recovery_strategy['actual_recovery_amount'].describe().unstack()\n\n# Lakukan Kruskal-Wallis test\nLevel_0_actual = era_900_1100.loc[df['recovery_strategy']=='Level 0 Recovery']['actual_recovery_amount']\nLevel_1_actual = era_900_1100.loc[df['recovery_strategy']=='Level 1 Recovery']['actual_recovery_amount']\nstats.kruskal(Level_0_actual,Level_1_actual) \n\n# Ulangi untuk rentang yang lebil kecil $950 - $1050\nera_950_1050 = df.loc[(df['expected_recovery_amount']<1050) & \n                      (df['expected_recovery_amount']>=950)]\nLevel_0_actual = era_950_1050.loc[df['recovery_strategy']=='Level 0 Recovery']['actual_recovery_amount']\nLevel_1_actual = era_950_1050.loc[df['recovery_strategy']=='Level 1 Recovery']['actual_recovery_amount']\nstats.kruskal(Level_0_actual,Level_1_actual)\n\nKruskalResult(statistic=30.246000000000038, pvalue=3.80575314300276e-08)"
  },
  {
    "objectID": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#regression-modeling-no-threshold",
    "href": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#regression-modeling-no-threshold",
    "title": "EDA Pada Kasus Pemulihan Hutang Bank",
    "section": "7. Regression modeling: no threshold",
    "text": "7. Regression modeling: no threshold\n\nKita sekarang ingin mengambil pendekatan berbasis regresi untuk memperkirakan dampak program pada threshold $1000 menggunakan data yang berada tepat di atas dan di bawah threshold.\n\n\nKita akan membuat dua model. Model pertama tidak memiliki threshold, sedangkan model kedua akan menyertakan threshold.\n\n\nModel pertama memprediksi actual recovery amount (variabel dependen) sebagai fungsi dari expected recovery amount (variabel independen). Diharapkan akan ada hubungan positif yang kuat antara kedua variabel ini.\n\n\nKita akan memeriksa penyesuaian R-kuadrat untuk melihat persentase varians yang dijelaskan oleh model. Dalam model ini, kita tidak memperlihatkan threshold tetapi hanya melihat bagaimana variabel yang digunakan untuk menetapkan pelanggan (expected recovery amount) berhubungan dengan variabel hasil (actual recovery amount).\n\n\n# Import statsmodels\nimport statsmodels.api as sm\n\n# Tentukan nilai X dan y\nX = era_900_1100['expected_recovery_amount']\ny = era_900_1100['actual_recovery_amount']\nX = sm.add_constant(X)\n\n# Buat model linear regression\nmodel = sm.OLS(y, X).fit()\npredictions = model.predict(X)\nmodel.summary()\n# Tampilkan rangkuman statistik model\n\n\n\nOLS Regression Results\n\n  Dep. Variable:    actual_recovery_amount   R-squared:             0.261\n\n\n  Model:                      OLS            Adj. R-squared:        0.256\n\n\n  Method:                Least Squares       F-statistic:           63.78\n\n\n  Date:                Sun, 06 Nov 2022      Prob (F-statistic): 1.56e-13\n\n\n  Time:                    05:42:26          Log-Likelihood:      -1278.9\n\n\n  No. Observations:            183           AIC:                   2562.\n\n\n  Df Residuals:                181           BIC:                   2568.\n\n\n  Df Model:                      1                                       \n\n\n  Covariance Type:         nonrobust                                     \n\n\n\n\n                              coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const                    -1978.7597   347.741    -5.690  0.000 -2664.907 -1292.612\n\n\n  expected_recovery_amount     2.7577     0.345     7.986  0.000     2.076     3.439\n\n\n\n\n  Omnibus:       64.493   Durbin-Watson:         1.777\n\n\n  Prob(Omnibus):  0.000   Jarque-Bera (JB):    185.818\n\n\n  Skew:           1.463   Prob(JB):           4.47e-41\n\n\n  Kurtosis:       6.977   Cond. No.           1.80e+04\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.8e+04. This might indicate that there arestrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#regression-modeling-adding-true-threshold",
    "href": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#regression-modeling-adding-true-threshold",
    "title": "EDA Pada Kasus Pemulihan Hutang Bank",
    "section": "8. Regression modeling: adding true threshold",
    "text": "8. Regression modeling: adding true threshold\n\nDari model pertama, kita melihat bahwa koefisien regresi dari “expected recovery amount” signifikan secara statistik.\n\n\nModel kedua menambahkan indikator threshold ke model (dalam hal ini di $1000).\n\n\nKita akan membuat variabel indikator (baik 0 atau 1) yang menunjukkan apakah jumlah pemulihan yang diharapkan lebih besar dari $1000 atau tidak. Ketika kita menambahkan threshold yang sebenarnya ke model, koefisien regresi untuk threshold yang sebenarnya mewakili jumlah tambahan yang dipulihkan karena strategi pemulihan yang lebih tinggi. Artinya, koefisien regresi untuk threshold sebenarnya mengukur ukuran diskontinuitas untuk pelanggan tepat di atas dan tepat di bawah ambang batas.\n\n\nJika strategi pemulihan yang lebih tinggi membantu memulihkan lebih banyak uang, maka koefisien regresi dari ambang batas yang sebenarnya akan lebih besar dari nol. Jika strategi pemulihan yang lebih tinggi tidak membantu memulihkan lebih banyak uang, maka koefisien regresi tidak akan signifikan secara statistik.\n\n\n# Create indicator (0 or 1) for expected recovery amount >= $1000\n# Buat indikator (0 atau 1) untuk expected recovery amount >= $1000\ndf['indicator_1000'] = np.where(df['expected_recovery_amount']<1000, 0, 1)\nera_900_1100 = df.loc[(df['expected_recovery_amount']<1100) & \n                      (df['expected_recovery_amount']>=900)]\n\n# Tentukan X and y\nX = era_900_1100[['expected_recovery_amount','indicator_1000']]\ny = era_900_1100['actual_recovery_amount']\nX = sm.add_constant(X)\n\n# Buat model linear regression \nmodel = sm.OLS(y,X).fit()\n\n# Tampilkan rangkuman statistik model\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:    actual_recovery_amount   R-squared:             0.314\n\n\n  Model:                      OLS            Adj. R-squared:        0.307\n\n\n  Method:                Least Squares       F-statistic:           41.22\n\n\n  Date:                Sun, 06 Nov 2022      Prob (F-statistic): 1.83e-15\n\n\n  Time:                    05:42:26          Log-Likelihood:      -1272.0\n\n\n  No. Observations:            183           AIC:                   2550.\n\n\n  Df Residuals:                180           BIC:                   2560.\n\n\n  Df Model:                      2                                       \n\n\n  Covariance Type:         nonrobust                                     \n\n\n\n\n                              coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const                        3.3440   626.274     0.005  0.996 -1232.440  1239.128\n\n\n  expected_recovery_amount     0.6430     0.655     0.981  0.328    -0.650     1.936\n\n\n  indicator_1000             277.6344    74.043     3.750  0.000   131.530   423.739\n\n\n\n\n  Omnibus:       65.977   Durbin-Watson:         1.906\n\n\n  Prob(Omnibus):  0.000   Jarque-Bera (JB):    186.537\n\n\n  Skew:           1.510   Prob(JB):           3.12e-41\n\n\n  Kurtosis:       6.917   Cond. No.           3.37e+04\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.37e+04. This might indicate that there arestrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#regression-modeling-adjusting-the-window",
    "href": "posts/2022-11-05-eda-pada-kasus-pemulihan-hutang-bank.html#regression-modeling-adjusting-the-window",
    "title": "EDA Pada Kasus Pemulihan Hutang Bank",
    "section": "9. Regression modeling: adjusting the window",
    "text": "9. Regression modeling: adjusting the window\n\nKoefisien regresi untuk threshold sebenarnya signifikan secara statistik dengan perkiraan dampak sekitar $278. Ini jauh lebih besar daripada $50 per pelanggan yang dibutuhkan untuk menjalankan strategi pemulihan yang lebih tinggi ini.\n\n\nUntuk menambahkan keyakinan bahwa hasil ini bukan karena memilih rentang expected recovery amount dari $900 hingga $1100. Analisis ini untuk rentang dari $950 hingga $1050 untuk melihat apakah kita mendapatkan hasil yang serupa.\n\n\nJawabannya? Dengan menggunakan rentang lebar ($900 hingga $1100) atau lebih sempit ($950 hingga $1050), jumlah pemulihan tambahan pada strategi pemulihan yang lebih tinggi jauh lebih besar daripada $50 per pelanggan yang dikeluarkan untuk strategi pemulihan yang lebih tinggi. Jadi kami menyimpulkan bahwa strategi pemulihan yang lebih tinggi sebanding dengan biaya tambahan $50 per pelanggan.\n\n\nera_950_1050 = df.loc[(df['expected_recovery_amount']<1050) & \n                      (df['expected_recovery_amount']>=950 )]\n\n# Tentukan X dan y \nX = era_950_1050[['expected_recovery_amount','indicator_1000']]\ny = era_950_1050['actual_recovery_amount']\nX = sm.add_constant(X)\n\n# Buat model linear regression \nmodel = sm.OLS(y,X).fit()\n\n# Tampilkan rangkuman statistik model\nmodel.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:    actual_recovery_amount   R-squared:             0.283\n\n\n  Model:                      OLS            Adj. R-squared:        0.269\n\n\n  Method:                Least Squares       F-statistic:           18.99\n\n\n  Date:                Sun, 06 Nov 2022      Prob (F-statistic): 1.12e-07\n\n\n  Time:                    05:42:26          Log-Likelihood:      -692.92\n\n\n  No. Observations:             99           AIC:                   1392.\n\n\n  Df Residuals:                 96           BIC:                   1400.\n\n\n  Df Model:                      2                                       \n\n\n  Covariance Type:         nonrobust                                     \n\n\n\n\n                              coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const                     -279.5243  1840.707    -0.152  0.880 -3933.298  3374.250\n\n\n  expected_recovery_amount     0.9189     1.886     0.487  0.627    -2.825     4.663\n\n\n  indicator_1000             286.5337   111.352     2.573  0.012    65.502   507.566\n\n\n\n\n  Omnibus:       39.302   Durbin-Watson:         1.955\n\n\n  Prob(Omnibus):  0.000   Jarque-Bera (JB):     82.258\n\n\n  Skew:           1.564   Prob(JB):           1.37e-18\n\n\n  Kurtosis:       6.186   Cond. No.           6.81e+04\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.81e+04. This might indicate that there arestrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2020-02-20-test.html",
    "href": "posts/2020-02-20-test.html",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "This notebook is a demonstration of some of capabilities of fastpages with notebooks.\nWith fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts!\n\n\nThe first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this:\n# \"My Title\"\n> \"Awesome summary\"\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\n\nSetting toc: true will automatically generate a table of contents\nSetting badges: true will automatically include GitHub and Google Colab links to your notebook.\nSetting comments: true will enable commenting on your blog post, powered by utterances.\n\nThe title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README.\n\n\n\nA #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post.\nA #hide_input comment at the top of any code cell will only hide the input of that cell.\n\n\nThe comment #hide_input was used to hide the code that produced this.\n\n\nput a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it:\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n\nput a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:\n\n\nCode\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\n\nplace a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it:\n\n#collapse-output\nprint('The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.')\n\nThe comment #collapse-output was used to collapse the output of this cell by default but you can expand it.\n\n\n\n\n\nCharts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook.\n\n\n\n# single-value selection over [Major_Genre, MPAA_Rating] pairs\n# use specific hard-wired values as the initial selected values\nselection = alt.selection_single(\n    name='Select',\n    fields=['Major_Genre', 'MPAA_Rating'],\n    init={'Major_Genre': 'Drama', 'MPAA_Rating': 'R'},\n    bind={'Major_Genre': alt.binding_select(options=genres), 'MPAA_Rating': alt.binding_radio(options=mpaa)}\n)\n  \n# scatter plot, modify opacity based on selection\nalt.Chart(df).mark_circle().add_selection(\n    selection\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(df).mark_circle().add_selection(\n    alt.selection_interval(bind='scales', encodings=['x'])\n).encode(\n    alt.X('Rotten_Tomatoes_Rating', type='quantitative'),\n    alt.Y('IMDB_Rating', type='quantitative', axis=alt.Axis(minExtent=30)),\n#     y=alt.Y('IMDB_Rating:Q', ), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\nYou can display tables per the usual way in your blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'Distributor', 'MPAA_Rating', 'IMDB_Rating', 'Rotten_Tomatoes_Rating']].head()\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide_Gross\n      Production_Budget\n      Distributor\n      MPAA_Rating\n      IMDB_Rating\n      Rotten_Tomatoes_Rating\n    \n  \n  \n    \n      0\n      The Land Girls\n      146083.0\n      8000000.0\n      Gramercy\n      R\n      6.1\n      NaN\n    \n    \n      1\n      First Love, Last Rites\n      10876.0\n      300000.0\n      Strand\n      R\n      6.9\n      NaN\n    \n    \n      2\n      I Married a Strange Person\n      203134.0\n      250000.0\n      Lionsgate\n      None\n      6.8\n      NaN\n    \n    \n      3\n      Let's Talk About Sex\n      373615.0\n      300000.0\n      Fine Line\n      None\n      NaN\n      13.0\n    \n    \n      4\n      Slam\n      1087521.0\n      1000000.0\n      Trimark\n      R\n      3.4\n      62.0\n    \n  \n\n\n\n\n\n\n\n\n\nYou can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax:\n![](my_icons/fastai_logo.png)\n\n\n\n\nRemote images can be included with the following markdown syntax:\n![](https://image.flaticon.com/icons/svg/36/36686.svg)\n\n\n\n\nAnimated Gifs work, too!\n![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif)\n\n\n\n\nYou can include captions with markdown images like this:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")"
  },
  {
    "objectID": "posts/2020-02-20-test.html#github-flavored-emojis",
    "href": "posts/2020-02-20-test.html#github-flavored-emojis",
    "title": "Fastpages Notebook Blog Post",
    "section": "GitHub Flavored Emojis",
    "text": "GitHub Flavored Emojis\nTyping I give this post two :+1:! will render this:\nI give this post two :+1:!"
  },
  {
    "objectID": "posts/2020-02-20-test.html#tweetcards",
    "href": "posts/2020-02-20-test.html#tweetcards",
    "title": "Fastpages Notebook Blog Post",
    "section": "Tweetcards",
    "text": "Tweetcards\nTyping > twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20"
  },
  {
    "objectID": "posts/2020-02-20-test.html#youtube-videos",
    "href": "posts/2020-02-20-test.html#youtube-videos",
    "title": "Fastpages Notebook Blog Post",
    "section": "Youtube Videos",
    "text": "Youtube Videos\nTyping > youtube: https://youtu.be/XfoYk_Z5AkI will render this:"
  },
  {
    "objectID": "posts/2020-02-20-test.html#boxes-callouts",
    "href": "posts/2020-02-20-test.html#boxes-callouts",
    "title": "Fastpages Notebook Blog Post",
    "section": "Boxes / Callouts",
    "text": "Boxes / Callouts\nTyping > Warning: There will be no second warning! will render this:\n\n\n\n\n\n\nWarning\n\n\n\nThere will be no second warning!\n\n\nTyping > Important: Pay attention! It's important. will render this:\n\n\n\n\n\n\nImportant\n\n\n\nPay attention! It’s important.\n\n\nTyping > Tip: This is my tip. will render this:\n\n\n\n\n\n\nTip\n\n\n\nThis is my tip.\n\n\nTyping > Note: Take note of this. will render this:\n\n\n\n\n\n\nNote\n\n\n\nTake note of this.\n\n\nTyping > Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs:\n\n\n\n\n\n\nNote\n\n\n\nA doc link to an example website: fast.ai should also work fine."
  },
  {
    "objectID": "posts/2020-02-20-test.html#footnotes",
    "href": "posts/2020-02-20-test.html#footnotes",
    "title": "Fastpages Notebook Blog Post",
    "section": "Footnotes",
    "text": "Footnotes\nYou can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this:\n{% raw %}For example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ 'This is the footnote.' | fndetail: 1 }}\n{{ 'This is the other footnote. You can even have a [link](www.github.com)!' | fndetail: 2 }}{% endraw %}\nFor example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ ‘This is the footnote.’ | fndetail: 1 }} {{ ‘This is the other footnote. You can even have a link!’ | fndetail: 2 }}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "jupyter\n\n\ncode\n\n\nanalysis\n\n\nEDA\n\n\nlinear regression\n\n\n\n\nExploratory data analysis untuk mengetahui efektivitas upaya pemulihan hutang bank dengan menggunakan scipy.\n\n\n\n\n\n\nNov 5, 2022\n\n\nI Made Nusa Yudiskara\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\ncode\n\n\nanalysis\n\n\nclassification\n\n\nlogistic regression\n\n\ndata cleaning\n\n\n\n\nArtikel ini membahas tentang kasus klasifikasi persetujuan permohonan aplikasi kartu kredit dan membangun model machine learning yang sesuai dengan kasus tersebut.\n\n\n\n\n\n\nOct 25, 2022\n\n\nI Made Nusa Yudiskara\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\ncode\n\n\nclassification\n\n\nimage recognition\n\n\ntensorflow\n\n\ndata cleaning\n\n\n\n\nArtikel ini membahas tentang cara sebuah gagasan dan pendekatan sederhana untuk membuat sebuah OCR (Optical Character Recognition) untuk melakukan extraksi text dari sebuah gambar catatan/halaman yang ditulis tangan.\n\n\n\n\n\n\nSep 18, 2022\n\n\nI Made Nusa Yudiskara\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-09-18-Optical-Character-Recognition.html",
    "href": "posts/2022-09-18-Optical-Character-Recognition.html",
    "title": "Optical Character Recognition Untuk Gambar Dengan Tulisan Tangan Menggunakan Tensorflow",
    "section": "",
    "text": "Optical Character Recognition adalah konversi otomatis teks dalam gambar menjadi kode huruf yang dapat digunakan dalam komputer dan aplikasi pemrosesan teks. Secara sederhana, ini adalah ekstraksi teks dari sebuah catatan/halaman dengan tulisan tangan dengan menggunakan alat seperti stylus, pensil ,pena ,dll.\n\n\n\n\n1 : Buat sebuah classifier digit (0-9) dan huruf (A-Z + a-z) menggunakan arsitektur cnn\n2 : Terapkan segmentasi pada setiap huruf pada kata yang telah ditulis\n3 : Klasifikasi setiap segment huruf kemudian dapatkan kata akhir dari gambar tersebut.\n\n::: {.cell _cell_guid=‘79c7e3d0-c299-4dcb-8224-4455121ee9b0’ _uuid=‘d629ff2d2480ee46fbb7e2d37f6b5fab8052498a’ execution=‘{“iopub.execute_input”:“2022-11-09T13:40:57.327363Z”,“iopub.status.busy”:“2022-11-09T13:40:57.326909Z”,“iopub.status.idle”:“2022-11-09T13:40:57.334193Z”,“shell.execute_reply”:“2022-11-09T13:40:57.333292Z”,“shell.execute_reply.started”:“2022-11-09T13:40:57.327330Z”}’ trusted=‘true’ execution_count=11}\n# Import Library\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os\nimport random \nimport cv2\nimport imutils\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras import backend as K\nfrom keras.layers import Dense, Activation, Flatten, Dense,MaxPooling2D, Dropout\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n:::\nDari data train hanya gunakan gambar (A-Z + a-z) dan 0-9. Kemudian ubah setiap nilai pixel pada gambar menjadi ke bentuk array\n\ndir = \"../input/handwritten-characters/Train/\"\ntrain_data = []\nimg_size = 32\nnon_chars = [\"#\",\"$\",\"&\",\"@\"]\nfor i in os.listdir(dir):\n    if i in non_chars:\n        continue\n    count = 0\n    sub_directory = os.path.join(dir,i)\n    for j in os.listdir(sub_directory):\n        count+=1\n        if count > 4000:\n            break\n        img = cv2.imread(os.path.join(sub_directory,j),0)\n        img = cv2.resize(img,(img_size,img_size))\n        train_data.append([img,i])\n\n\nlen(train_data)\n\n140000\n\n\nLakukan hal yang sama pada data validation\n\nval_dir = \"../input/handwritten-characters/Validation/\"\nval_data = []\nimg_size = 32\nfor i in os.listdir(val_dir):\n    if i in non_chars:\n        continue\n    count = 0\n    sub_directory = os.path.join(val_dir,i)\n    for j in os.listdir(sub_directory):\n        count+=1\n        if count > 1000:\n            break\n        img = cv2.imread(os.path.join(sub_directory,j),0)\n        img = cv2.resize(img,(img_size,img_size))\n        val_data.append([img,i])\n\n\nlen(val_data)\n\n15209\n\n\n\n# Shuffle atau acak setiap data yang ada pada train dan validation\nrandom.shuffle(train_data)\nrandom.shuffle(val_data)\n\n\n# Pisahkan nilai fitur dan label pada data train menjadi train_X untuk fitur dan train_Y untuk label\ntrain_X = []\ntrain_Y = []\nfor features,label in train_data:\n    train_X.append(features)\n    train_Y.append(label)\n\n\n# Pisahkan nilai fitur dan label pada data validation menjadi val_X untuk fitur dan val_Y untuk label\nval_X = []\nval_Y = []\nfor features,label in val_data:\n    val_X.append(features)\n    val_Y.append(label)\n\nTransformasi label ke bentuk biner dengan menggunakan LabelBinarizer() fungsi untuk mempercepat proses komputasi Pada kasus ini terdapat 62 : * A = [1, 0, 0, … ,0] * B = [0, 1, 0, … ,0] * C = [0, 0, 1, … ,0] * dst\n\nLB = LabelBinarizer()\ntrain_Y = LB.fit_transform(train_Y)\nval_Y = LB.fit_transform(val_Y)\n\n\ntrain_X = np.array(train_X)/255.0 # Ubah tipe data train_X dari list menjadi numpy.array dan ubah rentang nilai pixel dari 0-255 menjadi 0-1\ntrain_X = train_X.reshape(-1,32,32,1) # Ubah dimensi numpy.array dari (jumlah array, 32, 32) menjadi (jumlah array, 32, 32, 1)\ntrain_Y = np.array(train_Y) # Ubah tipe data train_Y dari list menjadi numpy.array\n\n\nval_X = np.array(val_X)/255.0 # Ubah tipe data val_X dari list menjadi numpy.array dan ubah rentang nilai pixel dari 0-255 menjadi 0-1\nval_X = val_X.reshape(-1,32,32,1) # Ubah dimensi numpy.array dari (jumlah array, 32, 32) menjadi (jumlah array, 32, 32, 1)\nval_Y = np.array(val_Y) # Ubah tipe data val_Y dari list menjadi numpy.array\n\n\nprint(train_X.shape,val_X.shape) # Tampilkan hasil reshape\n\n(140000, 32, 32, 1) (15209, 32, 32, 1)\n\n\n\nprint(train_Y.shape,val_Y.shape) # Tampilkan hasil reshape\n\n(140000, 35) (15209, 35)\n\n\nBangun model arsitektur neural network yang dibuat. Pada kasus ini, karena kita menghadapi kasus image recognition maka saya akan menggunakan model sequential menggunakan CNN(Convolutional Neural Network)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(32,32,1)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n \nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(35, activation='softmax'))\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 32, 32, 32)        320       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 5, 5, 128)         73856     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 2, 2, 128)         0         \n_________________________________________________________________\ndropout (Dropout)            (None, 2, 2, 128)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 512)               0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               65664     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 35)                4515      \n=================================================================\nTotal params: 162,851\nTrainable params: 162,851\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy'])\n\n\nhistory = model.fit(train_X,train_Y, epochs=50, batch_size=32, validation_data = (val_X, val_Y),  verbose=1)\n\nEpoch 1/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.6208 - accuracy: 0.8020 - val_loss: 0.2941 - val_accuracy: 0.9089\nEpoch 2/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.3500 - accuracy: 0.8819 - val_loss: 0.3439 - val_accuracy: 0.8565\nEpoch 3/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.3044 - accuracy: 0.8957 - val_loss: 0.2723 - val_accuracy: 0.9011\nEpoch 4/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.2779 - accuracy: 0.9039 - val_loss: 0.2713 - val_accuracy: 0.8966\nEpoch 5/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.2639 - accuracy: 0.9083 - val_loss: 0.2434 - val_accuracy: 0.9143\nEpoch 6/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.2493 - accuracy: 0.9123 - val_loss: 0.2556 - val_accuracy: 0.8985\nEpoch 7/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.2389 - accuracy: 0.9159 - val_loss: 0.2454 - val_accuracy: 0.9066\nEpoch 8/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.2319 - accuracy: 0.9179 - val_loss: 0.2186 - val_accuracy: 0.9262\nEpoch 9/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.2238 - accuracy: 0.9208 - val_loss: 0.2383 - val_accuracy: 0.9112\nEpoch 10/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.2207 - accuracy: 0.9211 - val_loss: 0.2343 - val_accuracy: 0.9181\nEpoch 11/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.2138 - accuracy: 0.9231 - val_loss: 0.2421 - val_accuracy: 0.9163\nEpoch 12/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.2090 - accuracy: 0.9247 - val_loss: 0.2305 - val_accuracy: 0.9211\nEpoch 13/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.2050 - accuracy: 0.9257 - val_loss: 0.2343 - val_accuracy: 0.9136\nEpoch 14/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.2039 - accuracy: 0.9256 - val_loss: 0.2425 - val_accuracy: 0.9108\nEpoch 15/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1992 - accuracy: 0.9274 - val_loss: 0.2435 - val_accuracy: 0.9128\nEpoch 16/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1964 - accuracy: 0.9288 - val_loss: 0.2448 - val_accuracy: 0.9131\nEpoch 17/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1952 - accuracy: 0.9284 - val_loss: 0.2377 - val_accuracy: 0.9224\nEpoch 18/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1895 - accuracy: 0.9294 - val_loss: 0.2416 - val_accuracy: 0.9217\nEpoch 19/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1877 - accuracy: 0.9304 - val_loss: 0.2437 - val_accuracy: 0.9211\nEpoch 20/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1848 - accuracy: 0.9310 - val_loss: 0.2383 - val_accuracy: 0.9223\nEpoch 21/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1833 - accuracy: 0.9326 - val_loss: 0.2464 - val_accuracy: 0.9152\nEpoch 22/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1839 - accuracy: 0.9316 - val_loss: 0.2564 - val_accuracy: 0.9116\nEpoch 23/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1805 - accuracy: 0.9325 - val_loss: 0.2408 - val_accuracy: 0.9220\nEpoch 24/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1818 - accuracy: 0.9327 - val_loss: 0.2503 - val_accuracy: 0.9175\nEpoch 25/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1793 - accuracy: 0.9332 - val_loss: 0.2443 - val_accuracy: 0.9227\nEpoch 26/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1770 - accuracy: 0.9336 - val_loss: 0.2488 - val_accuracy: 0.9200\nEpoch 27/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1749 - accuracy: 0.9345 - val_loss: 0.2566 - val_accuracy: 0.9070\nEpoch 28/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1773 - accuracy: 0.9334 - val_loss: 0.2423 - val_accuracy: 0.9250\nEpoch 29/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1756 - accuracy: 0.9345 - val_loss: 0.2576 - val_accuracy: 0.9145\nEpoch 30/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1731 - accuracy: 0.9362 - val_loss: 0.2417 - val_accuracy: 0.9228\nEpoch 31/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1717 - accuracy: 0.9358 - val_loss: 0.2507 - val_accuracy: 0.9208\nEpoch 32/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1743 - accuracy: 0.9349 - val_loss: 0.2572 - val_accuracy: 0.9119\nEpoch 33/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1710 - accuracy: 0.9361 - val_loss: 0.2564 - val_accuracy: 0.9172\nEpoch 34/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1715 - accuracy: 0.9355 - val_loss: 0.2544 - val_accuracy: 0.9172\nEpoch 35/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1689 - accuracy: 0.9365 - val_loss: 0.2629 - val_accuracy: 0.9195\nEpoch 36/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1695 - accuracy: 0.9359 - val_loss: 0.2575 - val_accuracy: 0.9166\nEpoch 37/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1696 - accuracy: 0.9366 - val_loss: 0.2614 - val_accuracy: 0.9175\nEpoch 38/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1685 - accuracy: 0.9362 - val_loss: 0.2812 - val_accuracy: 0.9041\nEpoch 39/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1645 - accuracy: 0.9378 - val_loss: 0.2544 - val_accuracy: 0.9197\nEpoch 40/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1666 - accuracy: 0.9377 - val_loss: 0.2621 - val_accuracy: 0.9172\nEpoch 41/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1671 - accuracy: 0.9374 - val_loss: 0.2648 - val_accuracy: 0.9181\nEpoch 42/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1639 - accuracy: 0.9380 - val_loss: 0.2651 - val_accuracy: 0.9087\nEpoch 43/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1652 - accuracy: 0.9383 - val_loss: 0.2764 - val_accuracy: 0.9122\nEpoch 44/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1649 - accuracy: 0.9380 - val_loss: 0.2562 - val_accuracy: 0.9250\nEpoch 45/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1654 - accuracy: 0.9379 - val_loss: 0.2498 - val_accuracy: 0.9273\nEpoch 46/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1650 - accuracy: 0.9387 - val_loss: 0.2623 - val_accuracy: 0.9233\nEpoch 47/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1629 - accuracy: 0.9388 - val_loss: 0.2570 - val_accuracy: 0.9244\nEpoch 48/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1623 - accuracy: 0.9392 - val_loss: 0.2640 - val_accuracy: 0.9185\nEpoch 49/50\n4375/4375 [==============================] - 12s 3ms/step - loss: 0.1619 - accuracy: 0.9394 - val_loss: 0.2802 - val_accuracy: 0.9105\nEpoch 50/50\n4375/4375 [==============================] - 13s 3ms/step - loss: 0.1630 - accuracy: 0.9395 - val_loss: 0.2605 - val_accuracy: 0.9200\n\n\nKode dibawah ini akan menampilkan plot tingkat akurasi model pada setiap epoch ketika diuji menggunakan data Train(garis biru) dan Validation(Garis Kuning)\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n\n\n\nKode dibawah ini akan menampilkan plot Loss model pada setiap epoch ketika diuji menggunakan data Train(garis biru) dan Validation(Garis Kuning)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\nFungsi sort contuour berfungsi untuk mendapatkan urutan yang tepat untuk setiap karakter atau huruf pada sebuah gambar. Pada kasus ini fungsi ini akan mencoba meng-extrack setiap huruf yang ada pada gambar sesuai urutan dari kiri ke kanan.\nFungsi get letter adalah mengumpulkan setiap contour huruf pada pada gambar dan melakukan prediksi dengan menggunakan model yang telah dibangun untuk setiap huruf sehingga membentuk sebuah kata\n\n\ndef sort_contours(cnts, method=\"left-to-right\"):\n    reverse = False\n    i = 0\n    if method == \"right-to-left\" or method == \"bottom-to-top\":\n        reverse = True\n    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n        i = 1\n    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n    key=lambda b:b[1][i], reverse=reverse))\n    return (cnts, boundingBoxes)\n\n\ndef get_letters(img):\n    letters = []\n    image = cv2.imread(img)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    ret,thresh1 = cv2.threshold(gray ,127,255,cv2.THRESH_BINARY_INV)\n    dilated = cv2.dilate(thresh1, None, iterations=2)\n\n    cnts = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n    # Untuk setiap contour lakukan prediksi huruf menggunakan model tensorflow\n    for c in cnts:\n        if cv2.contourArea(c) > 10:\n            (x, y, w, h) = cv2.boundingRect(c)\n            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        roi = gray[y:y + h, x:x + w]\n        thresh = cv2.threshold(roi, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n        thresh = cv2.resize(thresh, (32, 32), interpolation = cv2.INTER_CUBIC)\n        thresh = thresh.astype(\"float32\") / 255.0\n        thresh = np.expand_dims(thresh, axis=-1)\n        thresh = thresh.reshape(1,32,32,1)\n        ypred = model.predict(thresh)\n        ypred = LB.inverse_transform(ypred)\n        [x] = ypred\n        letters.append(x)\n    return letters, image\n\n\n# Fungsi ini dibuat untuk menggabungkan semua list prediksi huruf menjadi sebuah kata\ndef get_word(letter):\n    word = \"\".join(letter)\n    return word\n\n\nletter,image = get_letters(\"../input/handwriting-recognition/train_v2/train/TRAIN_00020.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)\n\nR0ML1N\n\n\n<matplotlib.image.AxesImage at 0x7f6924668710>\n\n\n\n\n\n\nletter,image = get_letters(\"../input/handwriting-recognition/train_v2/train/TRAIN_00030.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)\n\nIAFFFLX\n\n\n<matplotlib.image.AxesImage at 0x7f69a00664d0>\n\n\n\n\n\n\nletter,image = get_letters(\"../input/handwriting-recognition/validation_v2/validation/VALIDATION_0005.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)\n\nRUPP\n\n\n<matplotlib.image.AxesImage at 0x7f69a0201550>\n\n\n\n\n\n\nletter,image = get_letters(\"../input/handwriting-recognition/test_v2/test/TEST_0007.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)\n\nVFLENTJNE\n\n\n<matplotlib.image.AxesImage at 0x7f69a0226590>\n\n\n\n\n\n\n\n\n\nPada tahap recognition sangat tergantung kepada contour detection dari library opencv, jadi jika opencv tidak dapat menemukan contour huruf maka metode ini akan gagal.\nAda banyak variasi huruf untuk setiap tulisan huruf, sehingga masih banyak contoh yang diperlukan untuk melakukan training pada model ini.\nModel ini tidak akan berhasil jika digunakan pada huruf jalan atau sambung.\n\n\n\n\nPada artikel ini kita telah membahas bagaimana pendekatan character segmentation dan classification dapat digunakan untuk membuat sebuah OCR (Optical Character Recognition) untuk tulisan tangan. Untuk meningkatkan performa model ini, lebih banyak dataset diperlukan untuk pada proses training untuk mendapatkan performa yang lebih baik.\n\n\n\n\nhttps://www.pyimagesearch.com/2020/08/24/ocr-handwriting-recognition-with-opencv-keras-and-tensorflow/\nhttps://www.pyimagesearch.com/2015/04/20/sorting-contours-using-python-and-opencv/"
  }
]